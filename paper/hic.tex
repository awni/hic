\section{Hierarchical Information Content}
\label{sec:hic}

We construct a measure for the structural complexity of a system. As motivated
in section~\ref{sec:finding_complexity}, the complexity should grow through the compositions of
ordered and disordered subsystems in a hierarchical structure. At each such
composition (whether it from order to disorder or disorder to order), the
complexity grows. Hence, we decompose the overall complexity as the sum of the
complexity at each composition.

We compute complexity at each composition as the difference in mutual
information between the higher-level system and that of the subsystem. We
denote by $X^{\ell}$ the state variable at the $\ell$-th level of the
hierarchy. The neighborhood of $X^\ell$ is the set $\calN(X^\ell)$ which
contains the elements used to construct the system at the next level. Following
\citet{simon1991architecture}, we define the span of a level $d^\ell =
\left|\mathcal{S}(X^\ell)\right|$ as the number of elements used to construct the variable $X^\ell$. For
now we assume the span to be constant with a value of two $d^\ell = 2$ for
every level. We later generalize to larger spans.

The complexity at the composition of level $\ell$ from level $\ell -1$ is
computed from the squared difference of the mutual information of the
span at each level:
\begin{equation}
  C_\ell = \left[ I(X^{\ell+1}; Y^{\ell+1}) - I(X^\ell; Y^\ell) \right]^2
\end{equation}
The mutual information can be computed from the Shannon entropy and conditional
entropy when $X^\ell$ is discrete or the differential entropy when $X^\ell$ is
continuous~\citep{cover1999elements}:
\begin{equation}
\label{eq:mutual_information}
I(X^\ell; Y^\ell) = H(X^\ell) - H(X^\ell \mid Y^\ell)
\end{equation}

We use the level complexities $C_\ell$ to construct the overall system complexity.
\begin{definition}[Hierarchical Information Content]
\label{def:hic}
  The Hierarchical Information Content (HIC) of a hierarchical system $S$ with
  $L$ levels is given by:
  \begin{equation}
    \label{eq:hic}
    \hic(S) = \sum_{l=1}^{L-1} C_\ell = \sum_{l=1}^{L-1} \left[ I(X^{\ell+1}; Y^{\ell+1}) - I(X^\ell; Y^\ell) \right]^2.
  \end{equation}
\end{definition}

\paragraph{Larger spans} Using a larger span requires generalizing the mutual
information to multiple variables. The interaction information is one such
generalization~\citep{mcgill1954multivariate}. The interaction information of a
set of variables is defined recursively as:

\subsection{Examples}

We attempt to build intuition for the definition of HIC through some simple
examples.

\begin{example}
  \label{ex:constant}
  As a first example, consider the state $S = [0, 0, \ldots, 0]$,
  a constant sequence of all $0$s. At any level for any neighborhood, the mutual
  information $I(X^\ell; Y^\ell) = 0$. Hence the terms $C_\ell = 0$ for all
  $\ell$ and the overall $\hic(S) = 0$.
\end{example}

\begin{example}
  \label{ex:uniform}
  Let $S = [x_1, x_2, \ldots]$ consist of a sequence of independent draws from a
  multinomial uniform distribution over $K$ categories. Assuming a span of $2$,
  the variable $X^\ell$ consists of $2^\ell$ of the $x_i$ primitives and hence
    can take on any of $K^{(2^\ell)}$ values. Consider the mutual information of level
  $\ell$:
  \begin{equation}
    I(X^\ell; Y^\ell) = H(X^\ell) - H(X^\ell \mid Y^{\ell}) = 2^\ell \log K - 2^\ell \log K = 0
  \end{equation}
  Hence all of the $C^\ell = 0$ and $\hic(S) = 0$.
\end{example}

As examples~\ref{ex:constant} and \ref{ex:uniform} show, HIC behaves as
expected when the system exhibits complete order or complete disorder. In both
of these examples the HIC was zero because all of the mutual informations were
themselves zero. In the following example, we see slightly more interesting
behavior, where the individual mutual informations are not zero, but the
resulting HIC is.

\begin{example}
  \label{ex:repeats}
  Let $S = [0, 1, 0, 1, \ldots]$ consist of a sequence of alternating zeros and
  ones.

  The first level $X^1$ takes on the two values $\{0, 1\}$ with equal
  probability hence $H(X^1) = \log 2$. However given the neighbor $Y^1$ the
  value of $X^1$ is deterministic and hence $H(X^1 \mid Y^1) = 0$. Thus the
  overall mutual information is $I(X^1; Y^1) = \log 2$.

  The second level $X^2$ takes on two values $\{[0, 1], [1, 0]\}$ with
  equal probability, and we have $H(X^2) = \log 2$. Again $X^2$ is fully
  determined by $Y^2$ thus $H(X^2 \mid Y^2) = 0$ and $I(X^2, Y^2) = \log 2$.
  Combining these to produce $C^2$ yields:
  \begin{equation}
    C^2 = (I(X^2; Y^2) - I(X^1; Y^1))^2 = (\log 2 - \log 2)^2 = 0.
  \end{equation}
  The above argument generalizes to all $\ell$ so $\hic(S) = 0$.
\end{example}

In example \ref{ex:repeats} we used overlaping windows to construct $X^2$. If
we had chosen disjoint windows the HIC would be small but nonzero. Depending on
the problem this may be desirable in that the alternating seuqence should be
considered slightly more complex than a constant sequence. This is where
subjectivitity comes in. The designer should select from these choices
based on the setting at hand.
