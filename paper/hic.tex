\section{Hierarchical Information Content}
\label{sec:hic}

We construct a measure for the structural complexity of a system. As motivated
in section~\ref{sec:finding_complexity}, we want the the complexity to grow at
the compositions of ordered and disordered subsystems in a hierarchical
structure. We decompose the overall complexity as the sum of the complexity at
each composition.

Assume we are given a hierarchical system with $L$ levels. We construct the
global system complexity from the local complexity computed at each of the
$L-1$ compositions between consecutive levels. The local complexity measures
are given by the difference in mutual information of the system at level
$\ell$ and that of the subsytem at level $\ell - 1$. We denote by $X^{\ell}$ the
state variable at the $\ell$-th level of the hierarchy. Following
\citet{simon1991architecture}, we define the span of a level as the
number of elements from the $\ell - 1$ subsystem used to construct the state
$X^\ell$. For now we assume the span to be constant with a value of two for
every level. We later generalize to larger spans.

As an example, the state variable $X^\ell$ could be a molecule in an artificial
chemistry constructed from two atoms. Alternatively, $X^{\ell}$ could be an $n
\times n$ state of a cellular automata constructed from two $(\nicefrac{n}{2})
\times n$ sub-states. As an example from nature, $X^{\ell}$ could be a string
of $2^\ell$ base pairs of DNA constructed from two substrings of length $2^{\ell
- 1}$.

The complexity at the $\ell$-th composition is computed from the squared
difference of the mutual information of the span between level $\ell$ and level
$\ell + 1$:
\begin{equation}
    C_\ell = \left[ I(X^{\ell + 1}; Y^{\ell + 1}) - I(X^{\ell}; Y^{\ell}) \right]^2
\end{equation}
The mutual information can be computed from the Shannon entropy and conditional
entropy when $X^\ell$ is discrete or the differential entropy when $X^\ell$ is
continuous~\citep{cover1999elements}:
\begin{equation}
\label{eq:mutual_information}
I(X^\ell; Y^\ell) = H(X^\ell) - H(X^\ell \mid Y^\ell).
\end{equation}
The mutual information is large when the $\ell$-th level is highly ordered and
small when it is highly disordered. Hence, the values $C_\ell$ will be large
exactly at the transtion points between order and disorder (or vice-versa).  We
use the level complexities $C_\ell$ to construct the overall system complexity.

\begin{definition}[Hierarchical Information Content]
\label{def:hic}
  The Hierarchical Information Content (HIC) of a hierarchical system $S$ with
  $L$ levels is given by:
  \begin{equation}
    \label{eq:hic}
    \hic(S) = \sum_{l=1}^{L-1} C_\ell = \sum_{l=1}^{L-1} \left[ I(X^{\ell+1}; Y^{\ell+1}) - I(X^\ell; Y^\ell) \right]^2.
  \end{equation}
\end{definition}

\paragraph{Generalizing to larger spans:} Using a larger span requires
generalizing the mutual information to multiple variables. The interaction
information is one such generalization~\citep{mcgill1954multivariate}. The
interaction information of a set of variables is defined recursively as:
\begin{equation}
    \label{eq:interaction_information}
    I(X_1^\ell; \ldots; X_{n+1}^\ell) = I(X_1^\ell; \ldots; X_n^\ell) - I(X_1^\ell; \ldots; X_n^\ell \mid X_{n+1}^\ell),
\end{equation}
where the conditional mutual information is given by:
\begin{equation}
    \label{eq:conditional_mutual_information}
    I(X; Y \mid Z) = \sum_{Z} P(Z) I(X; Y).
\end{equation}

\paragraph{Estimating information criteria:} Computing the mutual information
in equation~\ref{eq:mutual_information} requires computing the entropy of the
state variables $H(X^\ell)$ and the conditional entropy $H(X^\ell \mid
Y^\ell)$. Estimating these quantities directly requires models for the
distributions $P(X^\ell)$ and $P(X^\ell \mid Y^\ell)$. For state variables with
a small domain these distributions can likely be estimated efficiently from
counts. However, for larger state variables sample efficiency becomes an issue.
In these cases more sophisticated density estimation techniques should be used.
Suitable techniques can be found in most modern statistical learning or machine
learning texts (see for example \citet{friedman2001elements}).

\paragraph{Specifying the hierarchy:} In some cases, the hierarchical structure
of the system may be self-evident. This might be the case if the system has a
natural tree-like structure. However, in the general case, both the boundaries
between levels and the submodules which make up the state at the next level up
must be specified. This is introduces a degree subjectivity to the HIC which
effects its ability to faithfully capture the complexity of the system.

As an example consider measuring the HIC of a sequence of DNA. We might define
the state $X^\ell$ to be a subsequence of $2^\ell$ nucleotides. On the other
hand there may be some natural structure in the DNA to take advantage of. For
example, codons are sequences of three nucleotides which each code for a
specific amino acid.  These would make a good definition for a state variable.

\subsection{Examples}

We attempt to build intuition for the definition of HIC through some simple
examples.

\begin{example}
  \label{ex:constant}
  As a first example, consider the state $S = [0, 0, \ldots, 0]$,
  a constant sequence of all $0$s. At any level for any neighborhood, the mutual
  information $I(X^\ell; Y^\ell) = 0$. Hence the terms $C_\ell = 0$ for all
  $\ell$ and the overall $\hic(S) = 0$.
\end{example}

\begin{example}
  \label{ex:uniform}
  Let $S = [x_1, x_2, \ldots]$ consist of a sequence of independent draws from a
  multinomial uniform distribution over $K$ categories. Assuming a span of $2$,
  the variable $X^\ell$ consists of $2^\ell$ of the $x_i$ primitives and hence
    can take on any of $K^{(2^\ell)}$ values. Consider the mutual information of level
  $\ell$:
  \begin{equation}
    I(X^\ell; Y^\ell) = H(X^\ell) - H(X^\ell \mid Y^{\ell}) = 2^\ell \log K - 2^\ell \log K = 0
  \end{equation}
  Hence all of the $C_\ell = 0$ and $\hic(S) = 0$.
\end{example}

As examples~\ref{ex:constant} and \ref{ex:uniform} show, the HIC behaves as
expected when the system exhibits complete order or complete disorder. In both
of these examples the HIC was zero because the mutual information at each level
was zero. In the following example, we see slightly more interesting behavior,
where the individual mutual informations are not zero, but the resulting HIC
is.

\begin{example}
  \label{ex:repeats}
  Let $S = [0, 1, 0, 1, \ldots]$ consist of a sequence of alternating zeros and
  ones.

  The first level $X^1$ takes on the two values $\{0, 1\}$ with equal
  probability hence $H(X^1) = \log 2$. However given the neighbor $Y^1$ the
  value of $X^1$ is deterministic and hence $H(X^1 \mid Y^1) = 0$. Thus the
  overall mutual information is $I(X^1; Y^1) = \log 2$.

  The second level $X^2$ takes on two values $\{[0, 1], [1, 0]\}$ with
  equal probability, and we have $H(X^2) = \log 2$. Again $X^2$ is fully
  determined by $Y^2$ thus $H(X^2 \mid Y^2) = 0$ and $I(X^2, Y^2) = \log 2$.
  Combining these to produce $C^2$ yields:
  \begin{equation}
    C_2 = (I(X^2; Y^2) - I(X^1; Y^1))^2 = (\log 2 - \log 2)^2 = 0.
  \end{equation}
  The above argument generalizes to all levels of the hierarchy so $\hic(S) = 0$.
\end{example}

In example \ref{ex:repeats} we used overlaping windows to construct $X^2$. If
we had chosen disjoint windows the HIC would be small but nonzero. Depending on
the problem this may be desirable in that the alternating seuqence should be
considered slightly more complex than a constant sequence. This is where
subjectivitity comes in. The designer should select from these choices
based on the setting at hand.

\subsection{Comparison to Statistical Complexity}
\label{statistical_complexity}

The statistical complexity of \citet{crutchfield1989inferring} measures the
complexity of a system by observing the number of distinct future distributions
given the present and past states.  Statistical complexity infers a set of
``causal states'' by collapsing the state variables representing the current
and past time into equivalence classes which all lead to indistinguishable
future distributions. The entropy of the distribution of these causal states
then serves as a measure of the complexity of the process. For futures which
are ordered or unchanging, the system will have one or a small number of causal
states and the entropy will be small. Similarly, for futures which are
uniformly random, the future distributions will be indistinguishable given the
past and present and the states will similarly collapse into a single or small
number of causal states. Thus random processes will also have low statistical
complexity.

Let $X_t$ represent the state variable at the $t$-th time step for a system
with $N$ state variables per time step. The past light cone
$\mathcal{X}_{\textrm{P}}$ of $X_t$ with a history of $T_{\textrm{P}}$ time
steps is given by:
\begin{equation}
    \mathcal{X}_{\textrm{P}} =
        \{X_{i, j} \mid i = t - 1, \ldots, t - T_{\textrm{P}}
            \;\;\land\;\; j = 1, \ldots, N
            \;\;\land\;\; X_{i, j} \rightarrow X_t \},
\end{equation}
where $A \rightarrow B$ means the state $A$ can influence $B$ (\emph{i.e.} $B$
is a function of $A$). The future light cone $\mathcal{X}_{\textrm{F}}$ of
$X_t$ with a future of $T_{\textrm{F}}$ time steps is similarly defined as:
\begin{equation}
    \mathcal{X}_{\textrm{F}} =
        \{X_{i, j} \mid i = t + 1, \ldots, t + T_{\textrm{F}}
            \;\;\land\;\; j = 1, \ldots, N
            \;\;\land\;\; X_t \rightarrow X_{i, j} \}.
\end{equation}
In words, the past light cone of $X_t$ is the set of states which can influence
$X_t$ and the future light cone is the set of states which can be influenced by
$X_t$.

Each past light cone and state variable $X_t$ defines a conditional
distribution over future light cones $P(\mathcal{X}_{\textrm{F}} \mid
\mathcal{X}_{\textrm{P}}, X_t)$. We can measure the similarity between to such
conditional distributions using a statistical divergence $D(P_i \| P_j)$. A
causal state $C$ represents a set of states $C = \{(\mathcal{X}_{\textrm{P}},
X_t)\}$ where $D(P_i \| P_j) = 0$ for every pair $(\mathcal{X}_{\textrm{P}},
X_t)_i$ and $(\mathcal{X}_{\textrm{P}}, X_t)_j$ in $C$. In other words, the
distribution of future light cones of $(\mathcal{X}_{\textrm{P}}, X_t)_i$ and
$(\mathcal{X}_{\textrm{P}}, X_t)_j$ are indistinguishable. In practice, we
relax $D(P_i \| P_j) < \tau$ for some predefined threshold due to finite
sample sizes. Furthermore, we only require that $D(P_i \| P_j) < \tau$ be
satisfied for a single $(\mathcal{X}_{\textrm{P}}, X_t)_j$ in $C$ instead of
all such states.

Unlike HIC, statistical complexity is a measure of process complexity. We can
use it to observe how the complexity of a system changes over time. We can also
use statistical complexity as a measure of the global complexity of the system
by applying it to every state at every time step; as we do in this work.

Perhaps more importantly, statistical complexity can be computed in many
cases without much difficulty. The distributions of future light cones given
past light cones and state variables can be estimated for reasonable values of
$T_\textrm{P}$ and $T_\textrm{F}$. \citet{shalizi2004quantifying} estimate the
local statistical complexity of circular cellular automata and show
qualitatively that the measure coorelates well to with the perceived complexity
of the automata.
