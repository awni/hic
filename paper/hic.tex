\section{Hierarchical Information Content}
\label{sec:hic}

We construct a measure for the structural complexity of a system. As motivated
in section~\ref{sec:finding_complexity}, we want the the complexity to grow at
the compositions of ordered and disordered subsystems in a hierarchical
structure. We decompose the overall complexity as the sum of the complexity at
each composition.

Assume we are given a hierarchical system with $L$ levels. We construct the
global system complexity from the local complexity computed at each of the
$L-1$ compositions between consecutive levels. The local complexity measures
are given by the difference in mutual information of the system at level
$\ell$ and that of the subsytem at level $\ell - 1$. We denote by $X^{\ell}$ the
state variable at the $\ell$-th level of the hierarchy. Following
\citet{simon1991architecture}, we define the span of a level as the
number of elements from the $\ell - 1$ subsystem used to construct the state
$X^\ell$. For now we assume the span to be constant with a value of two for
every level. We later generalize to larger spans.

As an example, the state variable $X^\ell$ could be a molecule in an artificial
chemistry constructed from two atoms. Alternatively, $X^{\ell}$ could be an $n
\times n$ state of a cellular automata constructed from two $(\nicefrac{n}{2})
\times n$ sub-states. As an example from nature, $X^{\ell}$ could be a string
of $2^\ell$ base pairs of DNA constructed from two substrings of length $2^{\ell
- 1}$.

The complexity at the $\ell$-th composition is computed from the squared
difference of the mutual information of the span between level $\ell$ and level
$\ell + 1$:
\begin{equation}
    C_\ell = \left[ I(X^{\ell + 1}; Y^{\ell + 1}) - I(X^{\ell}; Y^{\ell}) \right]^2
\end{equation}
The mutual information can be computed from the Shannon entropy and conditional
entropy when $X^\ell$ is discrete or the differential entropy when $X^\ell$ is
continuous~\citep{cover1999elements}:
\begin{equation}
\label{eq:mutual_information}
I(X^\ell; Y^\ell) = H(X^\ell) - H(X^\ell \mid Y^\ell).
\end{equation}
The mutual information is large when the $\ell$-th level is highly ordered and
small when it is highly disordered. Hence, the values $C_\ell$ will be large
exactly at the transtion points between order and disorder (or vice-versa).  We
use the level complexities $C_\ell$ to construct the overall system complexity.

\begin{definition}[Hierarchical Information Content]
\label{def:hic}
  The Hierarchical Information Content (HIC) of a hierarchical system $S$ with
  $L$ levels is given by:
  \begin{equation}
    \label{eq:hic}
    \hic(S) = \sum_{l=1}^{L-1} C_\ell = \sum_{l=1}^{L-1} \left[ I(X^{\ell+1}; Y^{\ell+1}) - I(X^\ell; Y^\ell) \right]^2.
  \end{equation}
\end{definition}

\paragraph{Generalizing to larger spans:} Using a larger span requires
generalizing the mutual information to multiple variables. The interaction
information is one such generalization~\citep{mcgill1954multivariate}. The
interaction information of a set of variables is defined recursively as:
\begin{equation}
    \label{eq:interaction_information}
    I(X_1^\ell; \ldots; X_{n+1}^\ell) = I(X_1^\ell; \ldots; X_n^\ell) - I(X_1^\ell; \ldots; X_n^\ell \mid X_{n+1}^\ell),
\end{equation}
where the conditional mutual information is given by:
\begin{equation}
    \label{eq:conditional_mutual_information}
    I(X; Y \mid Z) = \sum_{Z} P(Z) I(X; Y).
\end{equation}

\paragraph{Estimating information criteria:} Computing the mutual information
in equation~\ref{eq:mutual_information} requires computing the entropy of the
state variables $H(X^\ell)$ and the conditional entropy $H(X^\ell \mid
Y^\ell)$. Estimating these quantities directly requires models for the
distributions $P(X^\ell)$ and $P(X^\ell \mid Y^\ell)$. For state variables with
a small domain these distributions can likely be estimated efficiently from
counts. However, for larger state variables sample efficiency becomes an issue.
In these cases more sophisticated density estimation techniques should be used.
Suitable techniques can be found in most modern statistical learning or machine
learning texts (see for example \citet{friedman2001elements}).

\paragraph{Specifying the hierarchy:} In some cases, the hierarchical structure
of the system may be self-evident. This might be the case if the system has a
natural tree-like structure. However, in the general case, both the boundaries
between levels and the submodules which make up the state at the next level up
must be specified. This is introduces a degree subjectivity to the HIC which
effects its ability to faithfully capture the complexity of the system.

As an example consider measuring the HIC of a sequence of DNA. We might define
the state $X^\ell$ to be a subsequence of $2^\ell$ nucleotides. On the other
hand there may be some natural structure in the DNA to take advantage of. For
example, codons are sequences of three nucleotides which each code for a
specific amino acid.  These would make a good definition for a state variable.

\subsection{Examples}

We attempt to build intuition for the definition of HIC through some simple
examples.

\begin{example}
  \label{ex:constant}
  As a first example, consider the state $S = [0, 0, \ldots, 0]$,
  a constant sequence of all $0$s. At any level for any neighborhood, the mutual
  information $I(X^\ell; Y^\ell) = 0$. Hence the terms $C_\ell = 0$ for all
  $\ell$ and the overall $\hic(S) = 0$.
\end{example}

\begin{example}
  \label{ex:uniform}
  Let $S = [x_1, x_2, \ldots]$ consist of a sequence of independent draws from a
  multinomial uniform distribution over $K$ categories. Assuming a span of $2$,
  the variable $X^\ell$ consists of $2^\ell$ of the $x_i$ primitives and hence
    can take on any of $K^{(2^\ell)}$ values. Consider the mutual information of level
  $\ell$:
  \begin{equation}
    I(X^\ell; Y^\ell) = H(X^\ell) - H(X^\ell \mid Y^{\ell}) = 2^\ell \log K - 2^\ell \log K = 0
  \end{equation}
  Hence all of the $C^\ell = 0$ and $\hic(S) = 0$.
\end{example}

As examples~\ref{ex:constant} and \ref{ex:uniform} show, the HIC behaves as
expected when the system exhibits complete order or complete disorder. In both
of these examples the HIC was zero because the mutual information at each level
was zero. In the following example, we see slightly more interesting behavior,
where the individual mutual informations are not zero, but the resulting HIC
is.

\begin{example}
  \label{ex:repeats}
  Let $S = [0, 1, 0, 1, \ldots]$ consist of a sequence of alternating zeros and
  ones.

  The first level $X^1$ takes on the two values $\{0, 1\}$ with equal
  probability hence $H(X^1) = \log 2$. However given the neighbor $Y^1$ the
  value of $X^1$ is deterministic and hence $H(X^1 \mid Y^1) = 0$. Thus the
  overall mutual information is $I(X^1; Y^1) = \log 2$.

  The second level $X^2$ takes on two values $\{[0, 1], [1, 0]\}$ with
  equal probability, and we have $H(X^2) = \log 2$. Again $X^2$ is fully
  determined by $Y^2$ thus $H(X^2 \mid Y^2) = 0$ and $I(X^2, Y^2) = \log 2$.
  Combining these to produce $C^2$ yields:
  \begin{equation}
    C^2 = (I(X^2; Y^2) - I(X^1; Y^1))^2 = (\log 2 - \log 2)^2 = 0.
  \end{equation}
  The above argument generalizes to all levels of the hierarchy so $\hic(S) = 0$.
\end{example}

In example \ref{ex:repeats} we used overlaping windows to construct $X^2$. If
we had chosen disjoint windows the HIC would be small but nonzero. Depending on
the problem this may be desirable in that the alternating seuqence should be
considered slightly more complex than a constant sequence. This is where
subjectivitity comes in. The designer should select from these choices
based on the setting at hand.
