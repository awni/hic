import collections
import numpy as np
import math
import random


def _ent_impl(counts):
    total = 0
    inds = 0
    if isinstance(counts, collections.Counter):
        counts = (c for _, c in counts.most_common())
    if isinstance(counts, dict):
        counts = list(counts.values())
    for c in counts:
        total += c
        if c == 0:
            continue
        inds += c * math.log(c)
    entropy = math.log(total) - (1 / total) * inds
    return entropy, total


def entropy(counts):
    """
    Compute the Shannon entropy of a multinomial distribution given counts in a
    iterable or a Counter
        H(x) = \sum_x p(x) \log (1 / p(x))
    """
    return _ent_impl(counts)[0]


def conditional_entropy(cond_counts):
    """
    Compute the conditional entropy of a conditional multinomial distribution
    given a list of lists of counts or Counters for x given each y:
        H(x|y) = \sum_y p(y) \sum_x p(x|y) \log (1 / p(x|y))
    """
    if isinstance(cond_counts, dict):
        cond_counts = list(cond_counts.values())
    cond_ents = [_ent_impl(xs) for xs in cond_counts]
    cond_ent_sum = sum(cond_ent_y * ny for cond_ent_y, ny in cond_ents)
    y_tot = sum(ny for _, ny in cond_ents)
    return cond_ent_sum / y_tot


def mutual_information(cond_counts):
    """
    Compute the mutual information I(X; Y):
        I(x, y) = H(x) - H(x|y)
    """
    if isinstance(cond_counts, dict):
        cond_counts = list(cond_counts.values())
    if isinstance(cond_counts[0], collections.Counter):
        cond_counts = [
            [c for _, c in counter.most_common()] for counter in cond_counts]
    counts = [sum(c) for c in zip(*cond_counts)]
    return entropy(counts) - conditional_entropy(cond_counts)


def kl(p, q, eps=0):
    """
    Compute the KL divergence between discrete distributions p and q specified
    as dictinoaries of counts.
    """
    ptot = sum(p.values())
    qtot = sum(q.values())
    Hp = entropy(p)
    xEnt = 0
    for k, pc in p.items():
        qc = q.get(k, eps)
        if qc != 0:
            xEnt += pc * math.log(qc)
        else:
            xEnt = -float("inf")
    xEnt /= ptot
    xEnt -= math.log(qtot)
    return -Hp - xEnt


def l2(p, q):
    """
    Compute the L2 distance between two multinomial distributions specified as
    dictionaries of counts.
    """
    totalp = sum(p.values())
    totalq = sum(q.values())
    p_pnq = sum((v / totalp - q.get(k, 0.0) / totalq)**2 for k, v in p.items())
    q = sum((v / totalq)**2 for k, v in q.items() if k not in p)
    return math.sqrt(p_pnq + q)


def generate_causal_states(future_given_past, divergence, threshold):
    """
    Causal states are generated by merging past cones with "similar"
    distributions over future cones.

    future_given_past: A dictionary of counters representing conditional counts
        of future light cones given past light cones.
    divergence: A distance measure between two discrete probability distributions.
    threshold: A scalar merging threshold on the divergence.
    """
    past_states = list(future_given_past.keys())
    random.shuffle(past_states)

    # Keeps track of the conditional probability distribution for each causal
    # state as a list of counts
    causal_states = [future_given_past[past_states[0]].copy()]
    for past_cone in past_states[1:]:
        # Compute distances to already constructed causal states
        dists = [divergence(future_given_past[past_cone], cond_probs)
                    for cond_probs in causal_states]
        # Find the argmin
        argmin, dist = min(enumerate(dists), key = lambda pair: pair[1])
        if dist < threshold:
            # Merge when the distance is less than the threshold
            causal_states[argmin].update(future_given_past[past_cone])
        else:
            # Otherwise create a new causal state
            causal_states.append(future_given_past[past_cone].copy())

    return causal_states


def statistical_complexity(future_given_past):
    """
    `future_given_past` should be a dictionary of counters containing the
    conditional counts for each future light cone keye'd by the given past
    light cone.
    """
    # Generate the set of causal states:
    causal_states = generate_causal_states(future_given_past, l2, 0.1)

    # Compute the entropy of the distribution of causal states:
    counts = [sum(t for _, t in c.most_common()) for c in causal_states]
    return entropy(counts)


def get_past_cone(past_states, location, neighbors_fn):
    """
    Get the past light cone given past states at the provided location.
    The neighbors_fn should take a state, location and radius and return the
    list of neighbors.
    """
    t = len(past_states)
    return tuple(neighbors_fn(state, location, t - i - 1)
                     for i, state in enumerate(past_states))


def get_future_cone(future_states, location, neighbors_fn):
    """
    Get the future light cone given future states at the provided location.
    The neighbors_fn should take a state, location and radius and return the
    list of neighbors.
    """
    return tuple(neighbors_fn(state, location, i+1)
                     for i, state in enumerate(future_states))
